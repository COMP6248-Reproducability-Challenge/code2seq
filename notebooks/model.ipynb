{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code2seq_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdPeGmNcoK4c5JwRAkDQ+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegarab/code2seq-reproducibility-challenge/blob/feat%2Fmodel/notebooks/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4I6iIiOHo3n",
        "colab_type": "text"
      },
      "source": [
        "## Code2Seq model\n",
        "\n",
        "This notebook is used to experiment with the implementation of the code2seq model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzob5ayHj9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchbearer\n",
        "from torchbearer import callbacks\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGe9h8oX72z",
        "colab_type": "text"
      },
      "source": [
        "This should all be moved to a global `code2seq.py` file that loads these parameters from a config-file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZneGT0YT7XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab from global config?\n",
        "# based on config.py from paper implementation\n",
        "DECODER_SIZE = _ \n",
        "NUM_DECODER_LAYERS = _\n",
        "RNN_SIZE = _\n",
        "TARGET_VOCAB_MAX_SIZE = _"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80RYKNALYC1H",
        "colab_type": "text"
      },
      "source": [
        "This should be placed in `model.py` once final so that it can be imported. I am reluctant to move it there until it is somewhat function to make the workflow a bit easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gnCzE6RqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, subtokens_input_dim, paths_input_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Pretty sure the subtokens have a single embedding even though they are shown as two in the architecture figure??\n",
        "        # ... and are concated as encode_token(value(v1)) and encode_token(value(vl)) - basically encode the first and last token\n",
        "        # All embeddings need to have the same dim sine they are concated before passed to the fc network?\n",
        "        # Need to look at thow these are handled during the forward pass... concat?\n",
        "        # z = tanh(Wh[encoder_path(v1,...,vl);encode_token(value(v1));encode_token(value(vl))])\n",
        "        self.embedding_subtokens = nn.Embedding(subtokens_input_dim, EMBEDDING_DIM) \n",
        "        self.embedding_paths = nn.Embedding(paths_input_dim, EMBEDDING_DIM)\n",
        "\n",
        "        # LSTM(input_size, hidden_size)\n",
        "        # This encoder is bidirectional. Not sure what the hidden size is defined as here\n",
        "        # Could be RNN_SIZE/2 from the paper implementation.. ? n in the paper?\n",
        "        # Pretty sure the input_size has to follow the output of the embeddings. \n",
        "        # This is also why I think they need to be the same for the two different embeddings\n",
        "        # Dropout?\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, RNN_SIZE/2, bidirectional=True, dropout=_)\n",
        "\n",
        "        # Linear(in_features, out_features)\n",
        "        # out_features is l in the paper? or vocab size?\n",
        "        # as far as I can see, they have disabled the bias in their dense layers\n",
        "        self.lin = nn.Linear(RNN_SIZE/2, TARGET_VOCAB_MAX_SIZE, bias=False)\n",
        "\n",
        "    def forward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70C8gkNBRtHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # DECODER_SIZE used in config. Is this k in the paper?\n",
        "        # input should be the output of the FC network.. but is it still vocab size after\n",
        "        # tanh activation and softmax?\n",
        "        self.lstm = nn.LSTM(TARGET_VOCAB_MAX_SIZE, DECODER_SIZE, num_layers=NUM_DECODER_LAYERS)\n",
        "\n",
        "        # as far as I can see, they have disabled the bias in their dense layers\n",
        "        # out_features here is set to be the vocab size?\n",
        "        self.lin = nn.Linear(DECODER_SIZE, TARGET_VOCAB_MAX_SIZE, bias=False)\n",
        "\n",
        "    def forward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgvCN61cuq0",
        "colab_type": "text"
      },
      "source": [
        "This should probably be part of a `Code2Seq` class that contains an encoder and decoder?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtMZDJRAXhbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Code2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Code2Seq, self).__init__()\n",
        "        self.encoder = Encoder(_, _)\n",
        "        self.decoder = Decoder(_, _)\n",
        "\n",
        "    def attention(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def foward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Kh4-TddR5a",
        "colab_type": "text"
      },
      "source": [
        "The fully connected network is supposed to use a `tanh` function and `softmax` output?\n",
        "\n",
        "$$p(y_t | y_{<t}, z_1, \\dots, z_n) = \\text{softmax}(W_s\\tanh(W_c[c_t;h_t]))$$\n",
        "$$\\mathbf{\\alpha^t} = \\text{softmax}(h_tW_a\\mathbf{z})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqdwbYzgdXME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}