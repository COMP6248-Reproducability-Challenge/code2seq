{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code2seq_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO74cDIMk2H5iwLMYDKm2B1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegarab/code2seq-reproducibility-challenge/blob/feat%2Fmodel/notebooks/code2seq_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4I6iIiOHo3n",
        "colab_type": "text"
      },
      "source": [
        "## Code2Seq model\n",
        "\n",
        "This notebook is used to experiment with the implementation of the code2seq model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzob5ayHj9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchbearer\n",
        "from torchbearer import callbacks\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from config import Config\n",
        "from loader import Code2SeqDataset\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGe9h8oX72z",
        "colab_type": "text"
      },
      "source": [
        "This should all be moved to a global `code2seq.py` file that loads these parameters from a config-file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZneGT0YT7XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config.get_debug_config(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY10hu38RMD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "64babb5c-0fe2-403c-fa2c-0af2e807d10a"
      },
      "source": [
        "test_set = Code2SeqDataset(config.TEST_PATH, config=config)\n",
        "#train_set = Code2SeqDataset(config.TRAIN_PATH, config=config)\n",
        "#val_set = Code2SeqDataset(config.VAL_PATH, config=config)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "#train_loader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "#val_loader = DataLoader(val_set, batch_size=config.BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num training samples: 691974\n",
            "Dictionaries loaded.\n",
            "Loaded subtoken vocab. size: 73906\n",
            "Loaded target word vocab. size: 11319\n",
            "Loaded nodes vocab. size: 323\n",
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/57088 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▍        | 8267/57088 [00:00<00:00, 82668.11it/s]\u001b[A\n",
            " 28%|██▊       | 15915/57088 [00:00<00:00, 80703.69it/s]\u001b[A\n",
            " 42%|████▏     | 23885/57088 [00:00<00:00, 80395.96it/s]\u001b[A\n",
            " 56%|█████▌    | 31868/57088 [00:00<00:00, 80224.03it/s]\u001b[A\n",
            " 66%|██████▋   | 37911/57088 [00:00<00:00, 70725.78it/s]\u001b[A\n",
            " 77%|███████▋  | 43804/57088 [00:00<00:00, 64250.75it/s]\u001b[A\n",
            " 93%|█████████▎| 53054/57088 [00:00<00:00, 70730.90it/s]\u001b[A\n",
            "100%|██████████| 57088/57088 [00:00<00:00, 72903.53it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80RYKNALYC1H",
        "colab_type": "text"
      },
      "source": [
        "This should be placed in `model.py` once final so that it can be imported. I am reluctant to move it there until it is somewhat function to make the workflow a bit easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gnCzE6RqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, subtoken_input_dim, nodes_vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Pretty sure the subtokens have a single embedding even though they are shown as two in the architecture figure??\n",
        "        # ... and are concated as encode_token(value(v1)) and encode_token(value(vl)) - basically encode the first and last token\n",
        "        # All embeddings need to have the same dim sine they are concated before passed to the fc network?\n",
        "        # Need to look at thow these are handled during the forward pass... concat?\n",
        "        # z = tanh(Wh[encoder_path(v1,...,vl);encode_token(value(v1));encode_token(value(vl))])\n",
        "        self.embedding_subtokens = nn.Embedding(subtoken_input_dim, config.EMBEDDINGS_SIZE) \n",
        "        self.embedding_paths = nn.Embedding(nodes_vocab_size, config.EMBEDDINGS_SIZE)\n",
        "\n",
        "        # LSTM(input_size, hidden_size)\n",
        "        # This encoder is bidirectional. Not sure what the hidden size is defined as here\n",
        "        # Could be RNN_SIZE/2 from the paper implementation.. ? n in the paper?\n",
        "        # Pretty sure the input_size has to follow the output of the embeddings. \n",
        "        # This is also why I think they need to be the same for the two different embeddings\n",
        "        # Dropout?\n",
        "        self.lstm = nn.LSTM(config.EMBEDDINGS_SIZE, config.RNN_SIZE//2, \n",
        "                            bidirectional=True, \n",
        "                            num_layers=2,\n",
        "                            dropout=(1 - config.RNN_DROPOUT_KEEP_PROB))\n",
        "\n",
        "        # Linear(in_features, out_features)\n",
        "        # out_features is l in the paper? or vocab size?\n",
        "        # as far as I can see, they have disabled the bias in their dense layers\n",
        "        self.lin = nn.Linear(config.RNN_SIZE//2, config.TARGET_VOCAB_MAX_SIZE, bias=False)\n",
        "\n",
        "    def forward(self, start_leaf, ast_path, end_leaf, target):\n",
        "        #batch, max_e, _ = start_leaf.size()\n",
        "        encode_start = self.embedding_subtokens(start_leaf.long())\n",
        "        encode_end   = self.embedding_subtokens(end_leaf.long())\n",
        "\n",
        "        encode_start = encode_start.sum(1)\n",
        "        encode_end = encode_end.sum(1)\n",
        "        print('ast', ast_path.size())\n",
        "\n",
        "        ast_embedding = self.embedding_paths(ast_path.long())\n",
        "        print('ast emb', ast_embedding.size())\n",
        "\n",
        "        #lengths = torch.tensor([(path==0).nonzero()[0].data \n",
        "                                #for path in ast_path])\n",
        "        #packed = nn.utils.rnn.pack_padded_sequence(ast_embedding, lengths)\n",
        "        lstm_output, (hidden, cell) = self.lstm(ast_embedding) \n",
        "        print(hidden.size())\n",
        "        # Get the last layer\n",
        "        hidden = hidden[-2:, :, :]\n",
        "        print(hidden.size())\n",
        "        print()\n",
        "        #hidden = hidden.transpose(0, 1)\n",
        "        #print(hidden.size())\n",
        "        hidden = hidden.contiguous().view(9, 1, -1)\n",
        "        print(hidden.size())\n",
        "        hidden = hidden.squeeze(1)\n",
        "        print(hidden.size())\n",
        "\n",
        "        print()\n",
        "        print(encode_start.size())\n",
        "        print(encode_end.size())\n",
        "        encode_all = torch.cat([hidden, encode_start, encode_end], dim=1)\n",
        "\n",
        "        encode_all = self.lin(encode_all)\n",
        "        encode_all = F.tanh(encode_all)\n",
        "\n",
        "        return encode_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70C8gkNBRtHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # DECODER_SIZE used in config. Is this k in the paper?\n",
        "        # input should be the output of the FC network.. but is it still vocab size after\n",
        "        # tanh activation and softmax?\n",
        "        self.lstm = nn.LSTM(config.TARGET_VOCAB_MAX_SIZE, config.DECODER_SIZE, num_layers=config.NUM_DECODER_LAYERS)\n",
        "\n",
        "        # as far as I can see, they have disabled the bias in their dense layers\n",
        "        # out_features here is set to be the vocab size?\n",
        "        self.lin = nn.Linear(config.DECODER_SIZE, config.TARGET_VOCAB_MAX_SIZE, bias=False)\n",
        "\n",
        "    def forward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgvCN61cuq0",
        "colab_type": "text"
      },
      "source": [
        "This should probably be part of a `Code2Seq` class that contains an encoder and decoder?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtMZDJRAXhbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Code2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Code2Seq, self).__init__()\n",
        "        self.encoder = Encoder(_, _)\n",
        "        self.decoder = Decoder(_, _)\n",
        "\n",
        "    def attention(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def foward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Kh4-TddR5a",
        "colab_type": "text"
      },
      "source": [
        "The fully connected network is supposed to use a `tanh` function and `softmax` output?\n",
        "\n",
        "$$p(y_t | y_{<t}, z_1, \\dots, z_n) = \\text{softmax}(W_s\\tanh(W_c[c_t;h_t]))$$\n",
        "$$\\mathbf{\\alpha^t} = \\text{softmax}(h_tW_a\\mathbf{z})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqdwbYzgdXME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}