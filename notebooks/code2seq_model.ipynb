{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code2seq_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMskm3qEdGJGlZqCRw7D9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegarab/code2seq-reproducibility-challenge/blob/feat%2Fmodel/notebooks/code2seq_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4I6iIiOHo3n",
        "colab_type": "text"
      },
      "source": [
        "## Code2Seq model\n",
        "\n",
        "This notebook is used to experiment with the implementation of the code2seq model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzob5ayHj9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchbearer\n",
        "from torchbearer import callbacks\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from config import Config\n",
        "from loader import Code2SeqDataset\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGe9h8oX72z",
        "colab_type": "text"
      },
      "source": [
        "This should all be moved to a global `code2seq.py` file that loads these parameters from a config-file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZneGT0YT7XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config.get_debug_config(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY10hu38RMD9",
        "colab_type": "code",
        "outputId": "7bfd54ad-395b-4190-84c5-aca3c35ba3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "test_set = Code2SeqDataset('test', config=config)\n",
        "#train_set = Code2SeqDataset(config.TRAIN_PATH, config=config)\n",
        "#val_set = Code2SeqDataset(config.VAL_PATH, config=config)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n",
        "#train_loader = DataLoader(train_set, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "#val_loader = DataLoader(val_set, batch_size=config.BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num training samples: 691974\n",
            "Dictionaries loaded.\n",
            "Loaded subtoken vocab. size: 73906\n",
            "Loaded target word vocab. size: 11319\n",
            "Loaded nodes vocab. size: 323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80RYKNALYC1H",
        "colab_type": "text"
      },
      "source": [
        "This should be placed in `model.py` once final so that it can be imported. I am reluctant to move it there until it is somewhat function to make the workflow a bit easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gnCzE6RqJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, subtoken_input_dim, nodes_vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # z = tanh(Wh[encoder_path(v1,...,vl);encode_token(value(v1));encode_token(value(vl))])\n",
        "        self.embedding_subtokens = nn.Embedding(subtoken_input_dim, config.EMBEDDINGS_SIZE) \n",
        "        self.embedding_paths = nn.Embedding(nodes_vocab_size, config.EMBEDDINGS_SIZE)\n",
        "\n",
        "        self.num_layers = 2\n",
        "        self.lstm = nn.LSTM(config.EMBEDDINGS_SIZE, config.RNN_SIZE//2, \n",
        "                            bidirectional=True, \n",
        "                            num_layers=self.num_layers,\n",
        "                            dropout=(1 - config.RNN_DROPOUT_KEEP_PROB),\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Linear(in_features, out_features)\n",
        "        # in_features is something else.. See last line in self.forward(*args, **kwargs)\n",
        "        self.lin = nn.Linear(config.RNN_SIZE//2, config.DECODER_SIZE, bias=False)\n",
        "\n",
        "    def forward(self, start_leaf, ast_path, end_leaf, target, start_leaf_mask, end_leaf_mask,\n",
        "                target_mask, context_mask, ast_path_lengths):\n",
        "        # (batch, max_context, max_name_parts, dim)\n",
        "        start_embed = self.embedding_subtokens(start_leaf.long())\n",
        "        end_embed = self.embedding_subtokens(end_leaf.long())\n",
        "\n",
        "        # (batch, max_contexts, max_path_length+1, dim)\n",
        "        path_embed = self.embedding_paths(ast_path.long())\n",
        "\n",
        "        # (batch, max_contexts, max_name_parts, 1)\n",
        "        end_leaf_mask = end_leaf_mask.unsqueeze(-1)\n",
        "        start_leaf_mask = start_leaf_mask.unsqueeze(-1)\n",
        "\n",
        "        # (batch, max_contexts, dim)\n",
        "        start_embed = torch.sum(start_embed * start_leaf_mask, dim=2)\n",
        "        end_embed = torch.sum(end_embed * end_leaf_mask, dim=2)\n",
        "\n",
        "        max_context = path_embed.size()[1]\n",
        "        # (batch * max_contexts, max_path_lenght+1, dim)\n",
        "        flat_paths = path_embed.view(-1, config.MAX_PATH_LENGTH, config.EMBEDDINGS_SIZE)\n",
        "\n",
        "        lstm_output, (hidden, cell) = self.lstm(flat_paths) \n",
        "        hidden = hidden[-self.num_layers:, :, :]\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        # (batch * max_contexts, rnn_size)\n",
        "        final_rnn_state = torch.reshape(hidden, (hidden.size()[0], -1))\n",
        "\n",
        "        # (batch, max_contexts, rnn_size)\n",
        "        path_aggregated = torch.reshape(final_rnn_state,\n",
        "                                        (-1, max_context, config.RNN_SIZE))\n",
        "\n",
        "\n",
        "        # (batch, max_contexts, dim * 2 + rnn_size\n",
        "        context_embed = torch.cat([start_embed, path_aggregated, end_embed], dim=-1)\n",
        "\n",
        "        # Input needs to be (batch, _) ?\n",
        "        context_embed = F.tanh(self.lin(context_embed))\n",
        "\n",
        "        return context_embed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70C8gkNBRtHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # DECODER_SIZE used in config. Is this k in the paper?\n",
        "        # input should be the output of the FC network.. but is it still vocab size after\n",
        "        # tanh activation and softmax?\n",
        "        self.lstm = nn.LSTM(config.TARGET_VOCAB_MAX_SIZE, config.DECODER_SIZE, num_layers=config.NUM_DECODER_LAYERS)\n",
        "\n",
        "        # as far as I can see, they have disabled the bias in their dense layers\n",
        "        # out_features here is set to be the vocab size?\n",
        "        self.lin = nn.Linear(config.DECODER_SIZE, config.TARGET_VOCAB_MAX_SIZE, bias=False)\n",
        "\n",
        "    def forward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AVNJxPAMo0K",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgvCN61cuq0",
        "colab_type": "text"
      },
      "source": [
        "This should probably be part of a `Code2Seq` class that contains an encoder and decoder?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtMZDJRAXhbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Code2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Code2Seq, self).__init__()\n",
        "        self.encoder = Encoder(_, _)\n",
        "        self.decoder = Decoder(_, _)\n",
        "\n",
        "    def attention(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def foward(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Kh4-TddR5a",
        "colab_type": "text"
      },
      "source": [
        "The fully connected network is supposed to use a `tanh` function and `softmax` output?\n",
        "\n",
        "$$p(y_t | y_{<t}, z_1, \\dots, z_n) = \\text{softmax}(W_s\\tanh(W_c[c_t;h_t]))$$\n",
        "$$\\mathbf{\\alpha^t} = \\text{softmax}(h_tW_a\\mathbf{z})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqdwbYzgdXME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}